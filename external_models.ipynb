{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from efc import EnergyBasedFlowClassifier\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "from anomaly_flow.utils.binary_processing import split_flag_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_SYNTHETIC = False\n",
    "APPLY_REAL = False\n",
    "SAME_SCALE = True\n",
    "DATASET = \"NF-BoT-IoT-v2-DDoS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_netflow = {\n",
    "    \"IPV4_SRC_ADDR\":                \"object\",\n",
    "    \"L4_SRC_PORT\":                  \"float32\",\n",
    "    \"IPV4_DST_ADDR\":                \"object\",\n",
    "    \"L4_DST_PORT\":                  \"float32\",\n",
    "    \"PROTOCOL\":                     \"float32\",\n",
    "    \"L7_PROTO\":                     \"float64\",\n",
    "    \"IN_BYTES\":                     \"float32\",\n",
    "    \"IN_PKTS\":                      \"float32\",\n",
    "    \"OUT_BYTES\":                    \"float32\",\n",
    "    \"OUT_PKTS\":                     \"float32\",\n",
    "    \"TCP_FLAGS\":                    \"int32\",\n",
    "    \"CLIENT_TCP_FLAGS\":             \"int32\",\n",
    "    \"SERVER_TCP_FLAGS\":             \"int32\",\n",
    "    \"FLOW_DURATION_MILLISECONDS\":   \"float32\",\n",
    "    \"DURATION_IN\":                  \"float32\",\n",
    "    \"DURATION_OUT\":                 \"float32\",\n",
    "    \"MIN_TTL\":                      \"float32\",\n",
    "    \"MAX_TTL\":                      \"float32\",\n",
    "    \"LONGEST_FLOW_PKT\":             \"float32\",\n",
    "    \"SHORTEST_FLOW_PKT\":            \"float32\",\n",
    "    \"MIN_IP_PKT_LEN\":               \"float32\",\n",
    "    \"MAX_IP_PKT_LEN\":               \"float32\",\n",
    "    \"SRC_TO_DST_SECOND_BYTES\":      \"float64\",\n",
    "    \"DST_TO_SRC_SECOND_BYTES\":      \"float64\",\n",
    "    \"RETRANSMITTED_IN_BYTES\":       \"float32\",\n",
    "    \"RETRANSMITTED_IN_PKTS\":        \"float32\",\n",
    "    \"RETRANSMITTED_OUT_BYTES\":      \"float32\",\n",
    "    \"RETRANSMITTED_OUT_PKTS\":       \"float32\",\n",
    "    \"SRC_TO_DST_AVG_THROUGHPUT\":    \"float32\",\n",
    "    \"DST_TO_SRC_AVG_THROUGHPUT\":    \"float32\",\n",
    "    \"NUM_PKTS_UP_TO_128_BYTES\":     \"float32\",\n",
    "    \"NUM_PKTS_128_TO_256_BYTES\":    \"float32\",\n",
    "    \"NUM_PKTS_256_TO_512_BYTES\":    \"float32\",\n",
    "    \"NUM_PKTS_512_TO_1024_BYTES\":   \"float32\",\n",
    "    \"NUM_PKTS_1024_TO_1514_BYTES\":  \"float32\",\n",
    "    \"TCP_WIN_MAX_IN\":               \"float32\",\n",
    "    \"TCP_WIN_MAX_OUT\":              \"float32\",\n",
    "    \"ICMP_TYPE\":                    \"float32\",\n",
    "    \"ICMP_IPV4_TYPE\":               \"float32\",\n",
    "    \"DNS_QUERY_ID\":                 \"float32\",\n",
    "    \"DNS_QUERY_TYPE\":               \"float32\",\n",
    "    \"DNS_TTL_ANSWER\":               \"float32\",\n",
    "    \"FTP_COMMAND_RET_CODE\":         \"float32\",\n",
    "    \"Attack\":                       \"object\",\n",
    "    \"Label\":                        \"float32\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_DROP = [\n",
    "    'IPV4_SRC_ADDR', \n",
    "    'IPV4_DST_ADDR', \n",
    "    'L7_PROTO', \n",
    "    'L4_SRC_PORT', \n",
    "    'L4_DST_PORT', \n",
    "    'FTP_COMMAND_RET_CODE',\n",
    "    'Attack'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./datasets/NF-UNSW-NB15-v2-downsample.csv.gz\",\n",
    "    dtype=dtypes_netflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 478055 entries, 0 to 478054\n",
      "Data columns (total 45 columns):\n",
      " #   Column                       Non-Null Count   Dtype  \n",
      "---  ------                       --------------   -----  \n",
      " 0   IPV4_SRC_ADDR                478055 non-null  object \n",
      " 1   L4_SRC_PORT                  478055 non-null  float32\n",
      " 2   IPV4_DST_ADDR                478055 non-null  object \n",
      " 3   L4_DST_PORT                  478055 non-null  float32\n",
      " 4   PROTOCOL                     478055 non-null  float32\n",
      " 5   L7_PROTO                     478055 non-null  float64\n",
      " 6   IN_BYTES                     478055 non-null  float32\n",
      " 7   IN_PKTS                      478055 non-null  float32\n",
      " 8   OUT_BYTES                    478055 non-null  float32\n",
      " 9   OUT_PKTS                     478055 non-null  float32\n",
      " 10  TCP_FLAGS                    478055 non-null  int32  \n",
      " 11  CLIENT_TCP_FLAGS             478055 non-null  int32  \n",
      " 12  SERVER_TCP_FLAGS             478055 non-null  int32  \n",
      " 13  FLOW_DURATION_MILLISECONDS   478055 non-null  float32\n",
      " 14  DURATION_IN                  478055 non-null  float32\n",
      " 15  DURATION_OUT                 478055 non-null  float32\n",
      " 16  MIN_TTL                      478055 non-null  float32\n",
      " 17  MAX_TTL                      478055 non-null  float32\n",
      " 18  LONGEST_FLOW_PKT             478055 non-null  float32\n",
      " 19  SHORTEST_FLOW_PKT            478055 non-null  float32\n",
      " 20  MIN_IP_PKT_LEN               478055 non-null  float32\n",
      " 21  MAX_IP_PKT_LEN               478055 non-null  float32\n",
      " 22  SRC_TO_DST_SECOND_BYTES      478055 non-null  float64\n",
      " 23  DST_TO_SRC_SECOND_BYTES      478055 non-null  float64\n",
      " 24  RETRANSMITTED_IN_BYTES       478055 non-null  float32\n",
      " 25  RETRANSMITTED_IN_PKTS        478055 non-null  float32\n",
      " 26  RETRANSMITTED_OUT_BYTES      478055 non-null  float32\n",
      " 27  RETRANSMITTED_OUT_PKTS       478055 non-null  float32\n",
      " 28  SRC_TO_DST_AVG_THROUGHPUT    478055 non-null  float32\n",
      " 29  DST_TO_SRC_AVG_THROUGHPUT    478055 non-null  float32\n",
      " 30  NUM_PKTS_UP_TO_128_BYTES     478055 non-null  float32\n",
      " 31  NUM_PKTS_128_TO_256_BYTES    478055 non-null  float32\n",
      " 32  NUM_PKTS_256_TO_512_BYTES    478055 non-null  float32\n",
      " 33  NUM_PKTS_512_TO_1024_BYTES   478055 non-null  float32\n",
      " 34  NUM_PKTS_1024_TO_1514_BYTES  478055 non-null  float32\n",
      " 35  TCP_WIN_MAX_IN               478055 non-null  float32\n",
      " 36  TCP_WIN_MAX_OUT              478055 non-null  float32\n",
      " 37  ICMP_TYPE                    478055 non-null  float32\n",
      " 38  ICMP_IPV4_TYPE               478055 non-null  float32\n",
      " 39  DNS_QUERY_ID                 478055 non-null  float32\n",
      " 40  DNS_QUERY_TYPE               478055 non-null  float32\n",
      " 41  DNS_TTL_ANSWER               478055 non-null  float32\n",
      " 42  FTP_COMMAND_RET_CODE         478055 non-null  float32\n",
      " 43  Label                        478055 non-null  float32\n",
      " 44  Attack                       478055 non-null  object \n",
      "dtypes: float32(36), float64(3), int32(3), object(3)\n",
      "memory usage: 93.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_df = pd.read_csv(\n",
    "    f\"./datasets/{DATASET}-downsample.csv.gz\",\n",
    "    dtype=dtypes_netflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.finfo(np.float32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack\n",
      "Benign            459044\n",
      "Exploits            6310\n",
      "Fuzzers             4462\n",
      "Generic             3312\n",
      "Reconnaissance      2556\n",
      "DoS                 1159\n",
      "Analysis             460\n",
      "Backdoor             434\n",
      "Shellcode            285\n",
      "Worms                 33\n",
      "Name: count, dtype: int64\n",
      "Using cached file: 2e1b49bdf7ef775ccf86409f36645a01.\n"
     ]
    }
   ],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(df['Attack'].value_counts())\n",
    "df.drop(FEATURES_TO_DROP, axis=1, inplace=True)\n",
    "df = df[df < threshold]\n",
    "df = split_flag_columns(df)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack\n",
      "DDoS      5499554\n",
      "Benign      40511\n",
      "Name: count, dtype: int64\n",
      "Creating column TCP_FLAGS_BIN\n",
      "Created column TCP_FLAGS_BIN\n",
      "Creating column CLIENT_TCP_FLAGS_BIN\n",
      "Created column CLIENT_TCP_FLAGS_BIN\n",
      "Creating column SERVER_TCP_FLAGS_BIN\n",
      "Created column SERVER_TCP_FLAGS_BIN\n",
      "Creating column URGENT_POINTER\n",
      "Created column URGENT_POINTER\n",
      "Creating column ACKNOWLEDGEMENT\n",
      "Created column ACKNOWLEDGEMENT\n",
      "Creating column PUSH\n",
      "Created column PUSH\n",
      "Creating column RESET\n",
      "Created column RESET\n",
      "Creating column SYNCHRONISATION\n",
      "Created column SYNCHRONISATION\n",
      "Creating column FIN\n",
      "Created column FIN\n",
      "Creating column CLIENT_URGENT_POINTER\n",
      "Created column CLIENT_URGENT_POINTER\n",
      "Creating column CLIENT_ACKNOWLEDGEMENT\n",
      "Created column CLIENT_ACKNOWLEDGEMENT\n",
      "Creating column CLIENT_PUSH\n",
      "Created column CLIENT_PUSH\n",
      "Creating column CLIENT_RESET\n",
      "Created column CLIENT_RESET\n",
      "Creating column CLIENT_SYNCHRONISATION\n",
      "Created column CLIENT_SYNCHRONISATION\n",
      "Creating column CLIENT_FIN\n",
      "Created column CLIENT_FIN\n",
      "Creating column SERVER_URGENT_POINTER\n",
      "Created column SERVER_URGENT_POINTER\n",
      "Creating column SERVER_ACKNOWLEDGEMENT\n",
      "Created column SERVER_ACKNOWLEDGEMENT\n",
      "Creating column SERVER_PUSH\n",
      "Created column SERVER_PUSH\n",
      "Creating column SERVER_RESET\n",
      "Created column SERVER_RESET\n",
      "Creating column SERVER_SYNCHRONISATION\n",
      "Created column SERVER_SYNCHRONISATION\n",
      "Creating column SERVER_FIN\n",
      "Created column SERVER_FIN\n"
     ]
    }
   ],
   "source": [
    "cross_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "cross_df.dropna(inplace=True)\n",
    "print(cross_df['Attack'].value_counts())\n",
    "cross_df.drop(FEATURES_TO_DROP, axis=1, inplace=True)\n",
    "cross_df = cross_df[cross_df < threshold]\n",
    "cross_df = split_flag_columns(cross_df)\n",
    "cross_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_x, cross_y = cross_df.drop(['Label', 'Unnamed: 0'], axis=1), cross_df['Label']\n",
    "\n",
    "X, y = df.drop(['Label'], axis=1), df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X, y, stratify=y,\n",
    "                                        test_size=0.33, random_state=42\n",
    "                                   )\n",
    "cross_x_train, cross_x_test, cross_y_train, cross_y_test = train_test_split(\n",
    "                                                                cross_x, cross_y,\n",
    "                                                                stratify=cross_y,\n",
    "                                                                test_size=0.9,\n",
    "                                                                random_state=42\n",
    "                                                           )\n",
    "\n",
    "# Reescale the models to train and test\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_SYNTHETIC is True:\n",
    "    print(\"Using Synthetic Data\")\n",
    "    synthetic_df = pd.read_parquet(\n",
    "        f\"./datasets/{DATASET}-synthetic.parquet\"\n",
    "    )\n",
    "    synthetic_df[\"Label\"] = 0\n",
    "    synthetic_x, synthetic_y = synthetic_df.drop(['Label'], axis=1), synthetic_df['Label']\n",
    "    synthetic_x = synthetic_x.to_numpy()\n",
    "\n",
    "    if SAME_SCALE is True:\n",
    "        synthetic_x = scaler.transform(synthetic_x)\n",
    " \n",
    "    X_train = np.concatenate((X_train, synthetic_x), axis=0)\n",
    "    y_train = np.concatenate((y_train, synthetic_y))\n",
    "\n",
    "if SAME_SCALE is True:\n",
    "    cross_x_train = scaler.transform(cross_x_train)\n",
    "    cross_x_test = scaler.transform(cross_x_test)\n",
    "else:\n",
    "    external_scaler = MinMaxScaler()\n",
    "    cross_x_train = external_scaler.fit_transform(cross_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_REAL is True:\n",
    "    X_train = np.concatenate((X_train, cross_x_train), axis=0)\n",
    "    y_train = np.concatenate((y_train, cross_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for each algorithm\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "param_grid_if = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'contamination': [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'max_iter': [200, 300, 400],\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "}\n",
    "\n",
    "param_grid_lof = {\n",
    "    'contamination': [0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "}\n",
    "\n",
    "param_grid_efc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "lr_classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "if_classifier = IsolationForest(random_state=42)\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "mlp_classifier = MLPClassifier(random_state=42)\n",
    "svm_classifier = SVC(random_state=42)\n",
    "lof_classifier = LocalOutlierFactor(novelty=True)\n",
    "efc_classifier = EnergyBasedFlowClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1sc = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# Create a dictionary of classifiers and their respective parameter grids\n",
    "classifiers = {\n",
    "    'Random Forest': (rf_classifier, param_grid_rf, 'f1', False),\n",
    "    'Logistic Regression': (lr_classifier, param_grid_lr, 'f1', False),\n",
    "    'Isolation Forest': (if_classifier, param_grid_if, 'roc_auc', True), \n",
    "    'eXtreme Gradient Boosting': (xgb_classifier, param_grid_xgb, 'f1', False),\n",
    "    'Multilayer Perceptron': (mlp_classifier, param_grid_mlp, 'f1', False),\n",
    "    #'Support Vector Machine': (svm_classifier, param_grid_svm, 'f1', False),\n",
    "    'Local Outlier Factor': (lof_classifier, param_grid_lof, f1sc, True),\n",
    "    'Energy Based Flow Classifier': (efc_classifier, param_grid_efc, 'f1', False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Accuracy on the test set: 0.9971\n",
      "F1-Score on the test set: 0.9637\n",
      "F2-Score on the test set: 0.9736\n",
      "Accuracy on the cross-evaluation set: 0.7488\n",
      "F1-Score on the cross-evaluation set: 0.8552\n",
      "F2-Score on the cross-evaluation set: 0.7871\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "9 fits failed out of a total of 18.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.84659052        nan 0.90761213        nan 0.9137292 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'C': 10, 'penalty': 'l2'}\n",
      "Accuracy on the test set: 0.9931\n",
      "F1-Score on the test set: 0.9171\n",
      "F2-Score on the test set: 0.9403\n",
      "Accuracy on the cross-evaluation set: 0.0061\n",
      "F1-Score on the cross-evaluation set: 0.0002\n",
      "F2-Score on the cross-evaluation set: 0.0001\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l:\\Experimentos\\anomaly-flow\\.env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Isolation Forest: {'contamination': 0.01, 'n_estimators': 50}\n",
      "Accuracy on the test set: 0.9506\n",
      "F1-Score on the test set: 0.0177\n",
      "F2-Score on the test set: 0.0131\n",
      "Accuracy on the cross-evaluation set: 0.0069\n",
      "F1-Score on the cross-evaluation set: 0.0000\n",
      "F2-Score on the cross-evaluation set: 0.0000\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best parameters for eXtreme Gradient Boosting: {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 150}\n",
      "Accuracy on the test set: 0.9971\n",
      "F1-Score on the test set: 0.9641\n",
      "F2-Score on the test set: 0.9733\n",
      "Accuracy on the cross-evaluation set: 0.0071\n",
      "F1-Score on the cross-evaluation set: 0.0006\n",
      "F2-Score on the cross-evaluation set: 0.0004\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best parameters for Multilayer Perceptron: {'activation': 'tanh', 'hidden_layer_sizes': (50, 25), 'max_iter': 200}\n",
      "Accuracy on the test set: 0.9961\n",
      "F1-Score on the test set: 0.9522\n",
      "F2-Score on the test set: 0.9717\n",
      "Accuracy on the cross-evaluation set: 0.0071\n",
      "F1-Score on the cross-evaluation set: 0.0000\n",
      "F2-Score on the cross-evaluation set: 0.0000\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best parameters for Local Outlier Factor: {'contamination': 0.01}\n",
      "Accuracy on the test set: 0.9515\n",
      "F1-Score on the test set: 0.0222\n",
      "F2-Score on the test set: 0.0163\n",
      "Accuracy on the cross-evaluation set: 0.0046\n",
      "F1-Score on the cross-evaluation set: 0.0003\n",
      "F2-Score on the cross-evaluation set: 0.0002\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best parameters for Energy Based Flow Classifier: {}\n",
      "Accuracy on the test set: 0.9519\n",
      "F1-Score on the test set: 0.6232\n",
      "F2-Score on the test set: 0.8051\n",
      "Accuracy on the cross-evaluation set: 0.9927\n",
      "F1-Score on the cross-evaluation set: 0.9963\n",
      "F2-Score on the cross-evaluation set: 0.9985\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each classifier using GridSearchCV\n",
    "for classifier_name, (classifier, param_grid, metric, outlier_discriminator) in classifiers.items():\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        classifier,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring=metric,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best parameters and the corresponding accuracy on the test set\n",
    "    print(f\"Best parameters for {classifier_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    cross_pred = grid_search.predict(cross_x_test)\n",
    "    \n",
    "    if outlier_discriminator is True:\n",
    "        y_pred[y_pred == 1] = 0\n",
    "        y_pred[y_pred == -1] = 1\n",
    "        cross_pred[cross_pred == 1] = 0\n",
    "        cross_pred[cross_pred == -1] = 1\n",
    "    \n",
    "    # Evaluate the performance on the same silo\n",
    "    accuracy_value = accuracy_score(y_test, y_pred)\n",
    "    f1_value = f1_score(y_test, y_pred)\n",
    "    f2_value = fbeta_score(y_test, y_pred, beta=2)\n",
    "\n",
    "    print(f\"Accuracy on the test set: {accuracy_value:.4f}\")\n",
    "    print(f\"F1-Score on the test set: {f1_value:.4f}\")\n",
    "    print(f\"F2-Score on the test set: {f2_value:.4f}\")\n",
    "\n",
    "    # Evaluate the performance on cross silos approach\n",
    "    accuracy_cross = accuracy_score(cross_y_test, cross_pred)\n",
    "    f1_cross = f1_score(cross_y_test, cross_pred)\n",
    "    f2_cross = fbeta_score(cross_y_test, cross_pred, beta=2)\n",
    "\n",
    "    print(f\"Accuracy on the cross-evaluation set: {accuracy_cross:.4f}\")\n",
    "    print(f\"F1-Score on the cross-evaluation set: {f1_cross:.4f}\")\n",
    "    print(f\"F2-Score on the cross-evaluation set: {f2_cross:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learning(y_test, preds):\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    rec = recall_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "    missrate = fn / (fn + tp)\n",
    "    fallout = fp / (fp + tn)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    f2_value = fbeta_score(y_test, preds, beta=2)\n",
    "\n",
    "    return acc, rec, prec, f1, mcc, missrate, fallout, auc, f2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "def calculate_reconstruction_loss(x, x_hat):\n",
    "    losses = np.mean(abs(x - x_hat), axis=1)  # MAE\n",
    "    return losses\n",
    "\n",
    "class AutoEncoder():\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self.x_train, self.y_train = train_data\n",
    "        self.x_test, self.y_test = test_data\n",
    "\n",
    "        self.num_features = self.x_test.shape[1]\n",
    "        print(self.num_features)\n",
    "        self.model = self.model()\n",
    "        print(\"> Loaded\", DATASET, \" | Trainset:\", self.x_train.shape, \" | Testset:\", self.x_test.shape)\n",
    "        print(\"> Train samples:\", self.y_train.shape[0], \" | Test samples:\", self.y_test.value_counts().to_string().replace(\"\\n\", \", \"))\n",
    "        self.threshold = 0\n",
    "\n",
    "    def model(self):\n",
    "        model = tf.keras.models.Sequential([\n",
    "            Input(shape=(self.num_features,)),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dense(16, activation=\"relu\"),\n",
    "            Dense(8, activation=\"relu\"),\n",
    "            Dense(4, activation=\"relu\"),\n",
    "            Dense(8, activation=\"relu\"),\n",
    "            Dense(16, activation=\"relu\"),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dense(self.num_features, activation=\"sigmoid\")\n",
    "        ])  \n",
    "        model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "        return model\n",
    "\n",
    "    def fit(self):\n",
    "        # training only on benign traffic\n",
    "        history = self.model.fit(self.x_train[self.y_train == 0], self.x_train[self.y_train == 0], \n",
    "                epochs=10, \n",
    "                batch_size=128, \n",
    "                shuffle=True\n",
    "                )\n",
    "        self.threshold = history.history[\"loss\"][-1]\n",
    "        print(\">>> Threshold:\", self.threshold)\n",
    "        return self.model.get_weights(), len(self.x_train), {}\n",
    "\n",
    "    def evaluate(self):\n",
    "        inference = self.model.predict(self.x_test)\n",
    "        loss = self.model.evaluate(self.x_test, self.x_test)\n",
    "        inference_loss = calculate_reconstruction_loss(self.x_test, inference)\n",
    "\n",
    "        y_pred = inference_loss > self.threshold\n",
    "\n",
    "        acc, rec, prec, f1, mcc, missrate, fallout, auc, f2_value = eval_learning(self.y_test, y_pred)\n",
    "\n",
    "        output_dict = {\"acc\": acc, \"rec\": rec, \"prec\": prec, \"f1\": f1, \"mcc\": mcc, \"missrate\": missrate,\n",
    "                \"fallout\": fallout, \"auc\": auc, \"f2-score\": f2_value}\n",
    "\n",
    "        print(output_dict)\n",
    "\n",
    "        return loss, len(self.x_test), output_dict\n",
    "\n",
    "    def cross_evaluation(self, cross_test_data):\n",
    "        x_validation, y_validation = cross_test_data\n",
    "\n",
    "        inference = self.model.predict(x_validation) \n",
    "        loss = self.model.evaluate(x_validation, x_validation)\n",
    "        inference_loss = calculate_reconstruction_loss(x_validation, inference)\n",
    "        y_pred = inference_loss > self.threshold\n",
    "        acc, rec, prec, f1, mcc, missrate, fallout, auc, f2_value = eval_learning(y_validation, y_pred)\n",
    "\n",
    "        output_dict = {\"acc\": acc, \"rec\": rec, \"prec\": prec, \"f1\": f1, \"mcc\": mcc, \"missrate\": missrate,\n",
    "                \"fallout\": fallout, \"auc\": auc, \"f2-score\": f2_value}\n",
    "\n",
    "        print(f\"Results:\\n{output_dict}\")\n",
    "\n",
    "        return loss, len(self.x_test), output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "> Loaded NF-BoT-IoT-v2-DDoS  | Trainset: (320296, 52)  | Testset: (157759, 52)\n",
      "> Train samples: 320296  | Test samples: Label, 0.0    151485, 1.0      6274\n",
      "Epoch 1/10\n",
      "2403/2403 [==============================] - 20s 3ms/step - loss: 0.0145\n",
      "Epoch 2/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0042\n",
      "Epoch 3/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0032\n",
      "Epoch 4/10\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.0028\n",
      "Epoch 5/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0026\n",
      "Epoch 6/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0026\n",
      "Epoch 7/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0025\n",
      "Epoch 8/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0025\n",
      "Epoch 9/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0025\n",
      "Epoch 10/10\n",
      "2403/2403 [==============================] - 6s 2ms/step - loss: 0.0024\n",
      ">>> Threshold: 0.002431360771879554\n",
      "4930/4930 [==============================] - 15s 2ms/step\n",
      "4930/4930 [==============================] - 13s 2ms/step - loss: 0.0034\n",
      "{'acc': 0.20400737834291546, 'rec': 1.0, 'prec': 0.047584737085605504, 'f1': 0.09084656429414363, 'mcc': 0.0902158256083926, 'missrate': 0.0, 'fallout': 0.8289599630326435, 'auc': 0.5855200184836782, 'f2-score': 0.19987893848163368}\n",
      "155815/155815 [==============================] - 276s 2ms/step\n",
      "155815/155815 [==============================] - 276s 2ms/step - loss: 193780300906496.0000\n",
      "Results:\n",
      "{'acc': 0.9926876115986594, 'rec': 1.0, 'prec': 0.9926876115986594, 'f1': 0.996330388988832, 'mcc': 0.0, 'missrate': 0.0, 'fallout': 1.0, 'auc': 0.5, 'f2-score': 0.9985289166132563}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(193780300906496.0,\n",
       " 157759,\n",
       " {'acc': 0.9926876115986594,\n",
       "  'rec': 1.0,\n",
       "  'prec': 0.9926876115986594,\n",
       "  'f1': 0.996330388988832,\n",
       "  'mcc': 0.0,\n",
       "  'missrate': 0.0,\n",
       "  'fallout': 1.0,\n",
       "  'auc': 0.5,\n",
       "  'f2-score': 0.9985289166132563})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder = AutoEncoder((X_train, y_train), (X_test, y_test))\n",
    "autoencoder.fit() \n",
    "autoencoder.evaluate()\n",
    "autoencoder.cross_evaluation((cross_x_test, cross_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
